# OpenAI Research: Why Language Models Hallucinate

**Source:** OpenAI Research Blog
**Date:** September 5, 2025
**URL:** https://openai.com/index/why-language-models-hallucinate/

## DEFINITION

Hallucinations are **plausible but false statements** generated by language models. They can show up in surprising ways, even for seemingly straightforward questions.

## ROOT CAUSES

### 1. Evaluation Methods Set Wrong Incentives

Current evaluation methods encourage guessing rather than honesty about uncertainty. The problem is analogous to multiple-choice tests where guessing has no penalty.

**The Guessing Problem:**
- If a model guesses a birthday, it has 1/365 chance of being right
- Saying "I don't know" guarantees zero points
- Over thousands of questions, guessing models score better on accuracy metrics
- This creates perverse incentives to hallucinate

### 2. Accuracy-Only Scoreboards

Most benchmarks only measure accuracy (% correct), creating a false dichotomy between right and wrong. This ignores the critical distinction between:
- **Accurate responses** (correct answer)
- **Errors** (wrong answer - hallucination)
- **Abstentions** (model admits uncertainty)

**Real-World Example (SimpleQA Eval):**

| Metric | gpt-5-thinking-mini | OpenAI o4-mini |
|--------|---------------------|----------------|
| Abstention rate | 52% | 1% |
| Accuracy rate | 22% | 24% |
| Error rate (hallucination) | 26% | 75% |

**Key Insight:** o4-mini has slightly higher accuracy but **3x higher hallucination rate** because it rarely abstains.

### 3. Next-Word Prediction Limitations

Language models learn through pretraining by predicting the next word. This creates fundamental limitations:

**What Models Learn Well:**
- Spelling (consistent patterns)
- Grammar (consistent patterns)
- Parentheses matching (consistent patterns)

**What Models Cannot Learn:**
- Arbitrary low-frequency facts (no pattern to learn)
- Information that is essentially random (like birthdays)
- Facts that cannot be predicted from patterns alone

**The Pet Birthday Analogy:**
- Image recognition can learn cat vs. dog because there are visual patterns
- But learning a pet's birthday from photos is impossible - birthdays are random
- Similarly, LLMs cannot reliably learn arbitrary facts that don't follow patterns

## OPENAI'S PROPOSED SOLUTION

### Better Evaluation Methods

1. **Penalize confident errors more than uncertainty**
2. **Give partial credit for appropriate expressions of uncertainty**
3. **Update accuracy-based evals to discourage guessing**

This is similar to standardized tests that use negative marking for wrong answers.

**OpenAI Model Spec states:** "It is better to indicate uncertainty or ask for clarification than provide confident information that may be incorrect."

## IMPLICATIONS FOR ECHO SYSTEM

### Design Principles:

1. **Reward Uncertainty Expression**
   - Design systems that value "I don't know" over confident guesses
   - Implement confidence scoring for all outputs
   - Never penalize appropriate abstention

2. **Distinguish Error Types**
   - Track abstentions separately from errors
   - Treat hallucinations as worse than uncertainty
   - Build metrics that capture this distinction

3. **Pattern vs. Fact Awareness**
   - Identify which queries require factual recall vs. pattern matching
   - Route factual queries to verified knowledge bases
   - Use RAG for arbitrary facts, not model memory

4. **Calibration Requirements**
   - Implement calibration testing for all model outputs
   - Ensure confidence scores correlate with actual accuracy
   - Flag overconfident responses for review

### Architectural Recommendations:

1. **Verification Layer**
   - Never trust model outputs for factual claims
   - Cross-reference against authoritative sources
   - Implement the Sherlock Hub evidence-tiering approach

2. **Uncertainty Propagation**
   - Track uncertainty through multi-step reasoning
   - Compound uncertainty appropriately
   - Alert when cumulative uncertainty exceeds threshold

3. **Abstention Training**
   - Fine-tune models to abstain appropriately
   - Reward honest uncertainty in training
   - Penalize confident errors in feedback loops
